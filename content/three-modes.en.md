---
title: "Three modes of coding with AI"
slug: "three-modes"
date: 2025-08-17
draft: true
---

For many people, the choice is between _using AI_ for coding (which is sometimes referred to as _vibe coding_), and _not using AI_. But it's a bit more complex than that.

Geoffrey Litt in his blog post [Enough AI copilots! We need AI HUDs](https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds) makes the distinction between a _copilot_ mode and an _HUD_ mode, and I like this analogy (also because I'm a flight simulation enthusiast). I'd just want to add a third one to the mix.

In my experience, there are three fundamentally different modes emerging in AI-assisted development. Each of them has its own pros and cons, and is suited for different tasks, or different skills.

## Vibe Coding: You're The Product Manager

Vibe coding can be described as asking the LLM to build an application, and iterating on it via chat, until it is ready for production.

To me, it's kind of like being a product manager. It's very cool to just care about the product requirements and delegate the implementation to someone else. AI can make amazing products that way.

If I need a working prototype for an application I'm thinking of, I sometimes bootstrap it using a vibe-coding approach (usually the UI part of the app).

## Copilot: You're The Team Lead

With the copilot approach, you're asking the LLM to build parts of an application, often using a plan approach, possibly with some implementation instructions. You review changes, fix manually what is easier to you than it is for the LLM, and approve or deny the changes.

You're basically a team lead. You have junior or midlevel developers who you can delegate most of the features to, as long as you know enough about their work be accountable for it. That means planning implementation with them first, code review and possibly manual testing.

But at the same time, you keep charge of some implementation, especially the most technically challenging details that you don't feel AI agents are ready for.

## HUD: You're an Augmented Individual

Finally, with the HUD approach, you use the LLM only as a tool to help you write code. This is typically what the "Tab" feature in Cursor offers, and is very good at.

{{< figure
src="/images/hud.jpg"
alt="Picture of a HUD"
caption="The HUD (heads-up display) gives valuable information to pilots, without them having to take their eyes off the sky. (Photo by Shawn from Airdrie, Canada, CC BY-SA 2.0, via Wikimedia Commons)">}}

You're in charge of everything and can't delegate much, but you have fantastic tools to help you make your task more efficient, robust, and enjoyable. You might not take advantage of all the features that AI offers, but at least you can't blame it for going the wrong direction.

## Use Cases For Each Approach

There is obviously nothing wrong with any of these approaches, and none of them is better than the other. It all depends on what it is used for.

At work, I've started noticing that we use the three approaches for different tasks.

Our team is composed of developers (mostly senior) and UI/UI designers. We all consider ourselves beginners with AI tools, but here is what I've noticed in how we use them:

- Most developers use the HUD approach, i.e. the Cursor "Tab" feature, to write code. Sometimes they use Cursor's agent mode to interact via a chat, but only for small tasks.

- Some developers (including me) experiment more with the copilot approach, via Claude Code. We don't apply it (yet) to all tasks, but at least we can plan features or review the code, even if we sometimes fallback to the HUD approach for the implementation.

- Developers don't really use the Vibe Coding approach, but the UI/UX designers love it! They use it to prototype features, get feedback from stakeholders or even users, and iterate much more quickly than on a static Figma design.

## New Skills To Learn

As I talked already in my post [Turning Claude Code Into My Best Design Partner](/design-partner), using AI coding tools (especially with the copilot approach) requires you to learn new skills such as planning features.

But on top of that, I noticed that dealing with Vibe Coded code is a specific skill to learn too. Whether you like the idea of Vibe Coding or not, at some point you'll need to deal with vibe code, whether it is from someone else, or from your past you who just wanted to prototype something quickly.

Usually, apps generated by Vibe Coding are not production-quality, not without the intervention of a developer. AI might create a very cool app, but shortly you'll need to find someone to maintain it, fix bugs or adding features. Will AI be ready for that role in a few years, months, or weeks? Nobody knowsâ€¦

Until then, when such situations happen, I need to take over the code, understand its architecture and making it easy to maintain for me, and for an AI agent.

## From Vibe Coded Code To Maintainable Code

Here are some examples of common issues when transitioning from vibe-coded prototype to maintainable code:

- AI created **wrong abstractions**, often too complex: it sometimes tries to make too generic components, because it assumes it/you might need to reuse it. Or passes down global config props to many components, instead of making it globally available (e.g. the URL of your API, or an API key).

- AI **stores state locally instead of globally**: some components handle their state locally while it should be global to the app; or the opposite, storing form values globally where they could belong to the form component only.

It's not that the AI makes mistakes in these cases. It has trouble to know the right approach, and without someone to tell it how to do things properly, it just takes a guess and does its best. I know that with experience, you can put some common rules in the initial prompt (or any other rules file).

At work, when we receive a working prototype from the UX/UI team, we usually don't even try to use it as is. We start from fresh, but it's very helpful to have a working prototype of what the feature must look like.

The reason why we don't use the code generated for the prototype is that it doesn't respect our full tech stack (e.g. we use MobX for state management, where the prototype relies on classic React states), and doesn't use the components from our design system (for that one I'm sure it won't be long to be possible).

To me, it isn't a problem, as reimplementing a prototype in a working app is not complex. We can sometimes reuse some small pieces of code from the prototype, such as animations.

AI can actually even help at this stage: Claude Code is good at understanding a codebase, refactoring it to make it cleaner (e.g. extract UI components in different files), and documenting it (generating a README.md or a CLAUDE.md file). The idea of this refactoring phase is not that much to help AI understand it (although I think it can't hurt), but more to help human developers to understand it, and guide AI agents in implementing next features, and fixing potential bugs.

## Conclusion

Can a nondeveloper build a fully functional app, thanks to Vibe Coding? Yes, but on the long term, it might not be sustainable.

Can an experienced developer use AI as a copilot a delegate a full feature to it? Yes, but at some point, they'll need to take over the code, and probably fix some issues manually.

Approaches are not mutually exclusive, and you can use them together.

We need to stop judging other people for using the tools they have at their disposal, that match their skills and needs.

And we need to stop thinking about AI as a silverbullet that will make developers obsolete.

Our job is changing, and we should try to embrace it, rather than fearing it.
